{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f47b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train-v2.0.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87de52d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    articles = []\n",
    "    \n",
    "    for article in data[\"data\"]:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "\n",
    "                if not qa[\"is_impossible\"]:\n",
    "                  answer = qa[\"answers\"][0][\"text\"]\n",
    "                \n",
    "                inputs = {\"context\": paragraph[\"context\"], \"question\": question, \"answer\": answer}\n",
    "\n",
    "            \n",
    "                articles.append(inputs)\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dfdbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = prepare_data(data)\n",
    "\n",
    "# Create a Dataframe\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dce55e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.optim import adam\n",
    "#import evaluate\n",
    "from torch.utils.data import Dataset,DataLoader,RandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import  T5ForConditionalGeneration,T5TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed33de92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saptarshimallikthakur/miniconda3/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = T5TokenizerFast.from_pretrained('t5-base')\n",
    "MODEL = T5ForConditionalGeneration.from_pretrained('t5-base',return_dict=True)\n",
    "OPTIMIZER = torch.optim.Adam(MODEL.parameters(),lr = 0.00001)\n",
    "Q_LEN = 512\n",
    "T_LEN = 128\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "MODEL = MODEL.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5556ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Dataset class from PyTorch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Defining a new class QA_Dataset that inherits from the Dataset class\n",
    "class QA_Dataset(Dataset):\n",
    "    # The constructor method for the class\n",
    "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
    "        self.tokenizer = tokenizer  # The tokenizer to be used\n",
    "        self.q_len = q_len  # The maximum length for the questions\n",
    "        self.t_len = t_len  # The maximum length for the answers\n",
    "        self.data = dataframe  # The dataframe containing the data\n",
    "        self.questions = self.data[\"question\"]  # The questions from the dataframe\n",
    "        self.context = self.data[\"context\"]  # The context from the dataframe\n",
    "        self.answer = self.data['answer']  # The answers from the dataframe\n",
    "\n",
    "    # Method to get the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.questions)  # Returns the number of questions in the dataset\n",
    "\n",
    "    # Method to get a specific item from the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]  # The question at the given index\n",
    "        context = self.context[idx]  # The context at the given index\n",
    "        answer = self.answer[idx]  # The answer at the given index\n",
    "\n",
    "        # Tokenizing the question and context with the given maximum length and padding\n",
    "        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n",
    "                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "        # Tokenizing the answer with the given maximum length and padding\n",
    "        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\", \n",
    "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "\n",
    "        # Creating a tensor from the tokenized answer's input ids\n",
    "        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n",
    "        # Replacing all 0s in the labels tensor with -100\n",
    "        labels[labels == 0] = -100\n",
    "\n",
    "        # Returning a dictionary with the tokenized question's input ids, attention mask, labels, and the tokenized answer's attention mask\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": labels,\n",
    "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806eab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "\n",
    "train_data, val_data = train_test_split(data.iloc[:1000], test_size=0.2, random_state=42)\n",
    "\n",
    "train_sampler = RandomSampler(train_data.index)\n",
    "val_sampler = RandomSampler(val_data.index)\n",
    "\n",
    "qa_dataset = QA_Dataset(TOKENIZER, data, Q_LEN, T_LEN)\n",
    "\n",
    "train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed06cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|███████████████████████| 200/200 [06:22<00:00,  1.91s/it]\n",
      "Validation batches: 100%|███████████████████████| 50/50 [01:28<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 -> Train loss: 0.11002866379916668\tValidation loss: 0.048757441826164725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|███████████████████████| 200/200 [06:35<00:00,  1.98s/it]\n",
      "Validation batches: 100%|███████████████████████| 50/50 [01:29<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 -> Train loss: 0.07740694308420643\tValidation loss: 0.03301481024478562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initializing the training and validation loss to 0\n",
    "train_loss = 0\n",
    "val_loss = 0\n",
    "\n",
    "# Initializing the count of training and validation batches to 0\n",
    "train_batch_count = 0\n",
    "val_batch_count = 0\n",
    "\n",
    "# Looping over the epochs\n",
    "for epoch in range(2):\n",
    "    # Setting the model to training mode\n",
    "    MODEL.train()\n",
    "    \n",
    "    # Looping over the training data loader\n",
    "    for batch in tqdm(train_loader, desc=\"Training batches\"):\n",
    "        # Moving the batch data to the device\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = MODEL(\n",
    "                          input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          labels=labels,\n",
    "                          decoder_attention_mask=decoder_attention_mask\n",
    "                        )\n",
    "\n",
    "        # Zeroing the gradients\n",
    "        OPTIMIZER.zero_grad()\n",
    "        \n",
    "        # Backward pass to calculate the gradients\n",
    "        outputs.loss.backward()\n",
    "        \n",
    "        # Updating the weights\n",
    "        OPTIMIZER.step()\n",
    "        \n",
    "        # Accumulating the training loss\n",
    "        train_loss += outputs.loss.item()\n",
    "        \n",
    "        # Incrementing the count of training batches\n",
    "        train_batch_count += 1\n",
    "    \n",
    "    # Setting the model to evaluation mode\n",
    "    MODEL.eval()\n",
    "    \n",
    "    # Looping over the validation data loader\n",
    "    for batch in tqdm(val_loader, desc=\"Validation batches\"):\n",
    "        # Moving the batch data to the device\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = MODEL(\n",
    "                          input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          labels=labels,\n",
    "                          decoder_attention_mask=decoder_attention_mask\n",
    "                        )\n",
    "\n",
    "        # Zeroing the gradients\n",
    "        OPTIMIZER.zero_grad()\n",
    "        \n",
    "        # Backward pass to calculate the gradients\n",
    "        outputs.loss.backward()\n",
    "        \n",
    "        # Updating the weights\n",
    "        OPTIMIZER.step()\n",
    "        \n",
    "        # Accumulating the validation loss\n",
    "        val_loss += outputs.loss.item()\n",
    "        \n",
    "        # Incrementing the count of validation batches\n",
    "        val_batch_count += 1\n",
    "        \n",
    "    # Printing the average training and validation loss for the epoch\n",
    "    print(f\"{epoch+1}/{2} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss/val_batch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43b8046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def predict_answer(context, question, ref_answer=None):\n",
    "    inputs = TOKENIZER(question, context, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n",
    "    \n",
    "    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
    "    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
    "\n",
    "    outputs = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "  \n",
    "    predicted_answer = TOKENIZER.decode(outputs.flatten(), skip_special_tokens=True)\n",
    "    \n",
    "    if ref_answer:\n",
    "        # Load the Bleu metric\n",
    "        bleu = evaluate.load(\"google_bleu\")\n",
    "        score = bleu.compute(predictions=[predicted_answer], \n",
    "                            references=[ref_answer])\n",
    "    \n",
    "        print(\"Context: \\n\", context)\n",
    "        print(\"\\n\")\n",
    "        print(\"Question: \\n\", question)\n",
    "        return {\n",
    "            \"Reference Answer: \": ref_answer, \n",
    "            \"Predicted Answer: \": predicted_answer, \n",
    "            \"BLEU Score: \": score\n",
    "        }\n",
    "    else:\n",
    "        return predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f06231a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      " Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "\n",
      "\n",
      "Question: \n",
      " When did Beyonce start becoming popular?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Reference Answer: ': 'in the late 1990s',\n",
       " 'Predicted Answer: ': 'late 1990s',\n",
       " 'BLEU Score: ': {'google_bleu': 0.3}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = data.iloc[0]['context']\n",
    "answer = data.iloc[0]['answer']\n",
    "question = data.iloc[0]['question']\n",
    "\n",
    "predict_answer(context, question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbda5568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      " The remaining band members recorded \"Independent Women Part I\", which appeared on the soundtrack to the 2000 film, Charlie's Angels. It became their best-charting single, topping the U.S. Billboard Hot 100 chart for eleven consecutive weeks. In early 2001, while Destiny's Child was completing their third album, Beyoncé landed a major role in the MTV made-for-television film, Carmen: A Hip Hopera, starring alongside American actor Mekhi Phifer. Set in Philadelphia, the film is a modern interpretation of the 19th century opera Carmen by French composer Georges Bizet. When the third album Survivor was released in May 2001, Luckett and Roberson filed a lawsuit claiming that the songs were aimed at them. The album debuted at number one on the U.S. Billboard 200, with first-week sales of 663,000 copies sold. The album spawned other number-one hits, \"Bootylicious\" and the title track, \"Survivor\", the latter of which earned the group a Grammy Award for Best R&B Performance by a Duo or Group with Vocals. After releasing their holiday album 8 Days of Christmas in October 2001, the group announced a hiatus to further pursue solo careers.\n",
      "\n",
      "\n",
      "Question: \n",
      " How many weeks did their single \"Independent Women Part I\" stay on top?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Reference Answer: ': 'eleven',\n",
       " 'Predicted Answer: ': 'eleven',\n",
       " 'BLEU Score: ': {'google_bleu': 1.0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = data.iloc[100]['context']\n",
    "answer = data.iloc[100]['answer']\n",
    "question = data.iloc[100]['question']\n",
    "\n",
    "predict_answer(context, question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c06240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d7aeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
